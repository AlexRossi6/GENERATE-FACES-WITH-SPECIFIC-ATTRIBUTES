{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNZOp4capQzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data\n",
        "import numpy as np\n",
        "celeba = np.load('drive/My Drive/celeba.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZsvDSwX-V1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.models import load_model\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo9ffckl6ITm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# definition of the discriminator\n",
        "def discriminator(input_shape=(64,64,3)):\n",
        "  model = Sequential()\n",
        "  \n",
        "  \n",
        "  model.add(Conv2D(64, (5,5), padding='same', input_shape=input_shape))\n",
        "  \n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "\n",
        "  # downsample to 32x32\n",
        "  model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "  \n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        " \n",
        "\n",
        "  # downsample to 16x16\n",
        "  model.add(Conv2D(128, (5,5), strides=(2,2),padding='same'))\n",
        "  \n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        " \n",
        "  # downsample to 8x8\n",
        "  model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "  \n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "\n",
        "  # downsample to 4x4\n",
        "  model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "  \n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  \n",
        "  # classifier\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  optim = Adam(lr=0.0002, beta_1=0.5)\n",
        "  \n",
        "  #compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99VhTS7-e_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#definition of the generator\n",
        "def generator(latent_dimension):\n",
        "    model = Sequential()\n",
        "    \n",
        "    \n",
        "    n_nodes = 512 * 4 * 4\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dimension))\n",
        "    \n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "   \n",
        "    model.add(Reshape((4, 4, 512)))\n",
        "    # upsample to 8X8\n",
        "    model.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # upsample to 16x16\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # upsample to 32x32\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # upsample to 64x64\n",
        "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2),padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    # output layer 64x64x3\n",
        "    model.add(Conv2D(3, (5,5), activation='tanh',padding='same'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT6vzG_l_IA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_gan(gen, dis):\n",
        "  \n",
        "  dis.trainable = False\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  #add generator\n",
        "  model.add(gen)\n",
        "  \n",
        "  #add the discriminator\n",
        "  model.add(dis)\n",
        "  \n",
        "  #compile model\n",
        "  optim = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optim)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RiQ9J8J_Scb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function used to generate batch of real images \n",
        "def generate_real_samples(dataset, n_batch):\n",
        "  \n",
        "  #n_batch random images from the dataset\n",
        "  ix = randint(0, dataset.shape[0], n_batch)\n",
        "  \n",
        "  X = dataset[ix]\n",
        "  \n",
        "  #map images into the range [-1,1]\n",
        "  X = (X.astype('float32')-127.5)/127.5\n",
        "  \n",
        "  y = ones((n_batch, 1))\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXr7Zp_z_dkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dimension, n_batch):\n",
        "  \n",
        "  # generate points in the latent space\n",
        "  lat_input = randn(latent_dimension * n_batch)\n",
        "  \n",
        "  # reshape into a batch of inputs for the network\n",
        "  lat_input = lat_input.reshape(n_batch, latent_dimension)\n",
        "  \n",
        "  return lat_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gmnqIog_j44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a batch of fake samples\n",
        "def generate_fake_samples(gen, latent_dimension, n_batch):\n",
        "  \n",
        "  # generate points in latent space\n",
        "  lat_input = generate_latent_points(latent_dimension, n_batch)\n",
        "  # predict outputs\n",
        "  X = gen.predict(lat_input)\n",
        "  \n",
        "  # fake class labels\n",
        "  y = zeros((n_batch, 1)) \n",
        "  return X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnES8gN2_l3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(gen, dis, gan, dataset,latent_dimension, n_epochs=50, n_batch=64):\n",
        "  \n",
        "  half_batch = int(n_batch / 2)\n",
        "  \n",
        "  #number of batches per epoch\n",
        "  bat_per_epo = int((dataset.shape[0] / n_batch))\n",
        "  \n",
        "  #list instantiated to store the losses\n",
        "  dloss_real = []\n",
        "  dloss_fake = []\n",
        "  gloss = []\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  for i in range(n_epochs):\n",
        "    \n",
        "    \n",
        "    for j in range(bat_per_epo):\n",
        "      \n",
        "      #generate a batch of real samples\n",
        "      X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\n",
        "      #update discriminator model weights\n",
        "      d_loss1, _ = dis.train_on_batch(X_real, y_real)\n",
        "        \n",
        "      #generate a batch of fake generated samples\n",
        "      X_fake, y_fake = generate_fake_samples(gen, latent_dimension, half_batch)\n",
        "        \n",
        "      #update discriminator model weights\n",
        "      d_loss2, _ = dis.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "      #prepare points in latent space as input for the generator\n",
        "      X_gan = generate_latent_points(latent_dimension, n_batch)\n",
        "      \n",
        "      #create inverted labels for the fake samples\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      \n",
        "      #update the generator \n",
        "      g_loss = gan.train_on_batch(X_gan, y_gan)\n",
        "      \n",
        "      #store the losses\n",
        "      dloss_real.append(d_loss1)\n",
        "      dloss_fake.append(d_loss2)\n",
        "      gloss.append(g_loss)\n",
        "\n",
        "      #print the epoch, batch number annd losses\n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "      (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "\n",
        "\n",
        "      \n",
        "    \n",
        "    #at the end of each epoch, save the models and the losses\n",
        "    losses = [np.array(dloss_real),np.array(dloss_fake),np.array(gloss)]\n",
        "    \n",
        "    save_training(i,gan, gen, dis, dataset, latent_dimension,losses)\n",
        "    \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU8k5jqnBYN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save a plot of n*n images\n",
        "def save_plot(examples, epoch, n=4):\n",
        "  \n",
        "  # scale from [-1,1] to [0,1]\n",
        "  examples = (examples + 1) / 2.0\n",
        "  \n",
        "  # plot images\n",
        "  for i in range(n * n):\n",
        "  \n",
        "  # define subplot\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "  # turn off axis\n",
        "    pyplot.axis('off')\n",
        "  # plot raw pixel data\n",
        "    pyplot.imshow(examples[i])\n",
        "  \n",
        "  # save plot to file\n",
        "  filename = 'drive/My Drive/model/generated_plot_e%03d.png' % (epoch+1)\n",
        "  pyplot.savefig(filename)\n",
        "  pyplot.close()\n",
        "\n",
        "\n",
        "\n",
        "#evaluate the discriminator, generate and save a plot of fake images, save models\n",
        "def save_training(epoch,gan, gen, dis, dataset, latent_dimension,losses, n_samples=16):\n",
        "  \n",
        "  # prepare real samples\n",
        "  X_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\n",
        "  # evaluate discriminator on real examples\n",
        "  _, acc_real = dis.evaluate(X_real, y_real, verbose=0)\n",
        "  \n",
        "  # prepare fake examples\n",
        "  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "  \n",
        "  # evaluate discriminator on fake examples\n",
        "  _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "  \n",
        "  # summarize discriminator performance\n",
        "  print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "  \n",
        "  # save plot\n",
        "  save_plot(x_fake, epoch)\n",
        "  \n",
        "  #save models\n",
        "  filename = 'drive/My Drive/model/generator_model_%03d.h5' % (epoch + 42)\n",
        "  gen.save(filename)\n",
        "  filename = 'drive/My Drive/model/gan_model_%03d.h5' % (1)\n",
        "  gan.save(filename)\n",
        "  filename = 'drive/My Drive/model/d_model_%03d.h5' % (1)\n",
        "  dis.trainable = True\n",
        "  \n",
        "  for layer in d_model.layers:\n",
        "    layer.trainable = True\n",
        "  d_model.save(filename)\n",
        "  d_model.trainable = False\n",
        "  for layer in d_model.layers:\n",
        "    layer.trainable = False\n",
        "  \n",
        "  np.savez('drive/My Drive/Losses/losses.npz', *losses)\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKFHQbYLBmzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training\n",
        "\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = discriminator()\n",
        "# create the generator\n",
        "g_model = generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# train model\n",
        "train(g_model, d_model, gan_model,celeba[:192599], latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeHDZi1-kcjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#restart training from saved models\n",
        "\n",
        "\n",
        "def load_models():\n",
        "    \n",
        "    filename = 'drive/My Drive/model/d_model_001.h5'\n",
        "    dis = load_model(filename, compile=True)\n",
        "    dis.trainable = True\n",
        "    \n",
        "    for layer in d_model.layers:\n",
        "        layer.trainable = True\n",
        "   \n",
        "    \n",
        "    filename = 'drive/My Drive/model/generator_model_041.h5'\n",
        "    gen = load_model(filename, compile=True)\n",
        "    \n",
        "    gan = define_gan(gen, dis)\n",
        "    gan.summary()\n",
        "    return dis, gen, gan\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcdeS3udB0iP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# create a plot of n*n generated images\n",
        "latent_dim = 100\n",
        "def plot_generated(examples, n):\n",
        "  # plot images\n",
        "  for i in range(n * n):\n",
        "    # define subplot\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "    # turn off axis\n",
        "    pyplot.axis('off')\n",
        "    # plot raw pixel data\n",
        "    pyplot.imshow(examples[i, :, :])\n",
        "    pyplot.show()\n",
        "# load model\n",
        "model = load_model('drive/My Drive/model/generator_model_040.h5')\n",
        "# generate images\n",
        "latent_points = generate_latent_points(latent_dim, 25)\n",
        "# generate images\n",
        "X = model.predict(latent_points)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "plot_generated(X, 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}