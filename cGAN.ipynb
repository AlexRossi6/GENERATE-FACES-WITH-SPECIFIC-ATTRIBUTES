{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cGan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzD4w9OzS64U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "celeba = np.load('drive/My Drive/celeba.npy')\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZaFuOrrU2uO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load attributes\n",
        "attribute = np.loadtxt('drive/My Drive/list_attr_celeba.txt',skiprows=2,unpack=True,usecols=range(1,41))\n",
        "attribute = np.transpose(attribute)\n",
        "attribute[attribute==-1]=0\n",
        "print(attribute.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRn7sDapTB2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, Activation, Lambda\n",
        "from tensorflow.keras.layers import Reshape, Multiply\n",
        "from tensorflow.keras.layers import Flatten, concatenate\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
        "from tensorflow.math import multiply\n",
        "from tensorflow.keras import backend\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBVaY97hTE3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def expand_label(x):\n",
        "  x = backend.expand_dims(x,axis=1)\n",
        "  x = backend.expand_dims(x,axis=1)\n",
        "  x = backend.tile(x,[1,32,32,1])\n",
        "  return x\n",
        "\n",
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(64,64,3),n_labels = 40):\n",
        "  \n",
        "  init = RandomNormal(stddev = 0.02)\n",
        "  \n",
        "  #label input\n",
        "  \n",
        "  in_label = Input(shape=(n_labels,))\n",
        "  \n",
        "  #image input\n",
        "  in_image = Input(shape=in_shape)\n",
        "\n",
        "  \n",
        "  label_input = Lambda(expand_label)(in_label)\n",
        "  \n",
        "  \n",
        "  \n",
        "  # downsample to 32x32\n",
        "  model = Conv2D(64, (5,5), padding='same',strides=(2,2),kernel_initializer=init,input_shape=in_shape)(in_image)\n",
        "  model = LeakyReLU(alpha=0.2)(model)\n",
        "  \n",
        "\n",
        "  label_input = Lambda(expand_label)(in_label)\n",
        "\n",
        "  #concatenate label input with the output of the first hidden layer\n",
        "  x = concatenate([model, label_input], axis=3)\n",
        "  \n",
        "  \n",
        "  model = Conv2D(128, (5,5),kernel_initializer=init, padding='same',input_shape=(32,32,104))(x)\n",
        "  model = LeakyReLU(alpha=0.2)(model)\n",
        "  \n",
        "\n",
        "  # downsample to 16x16\n",
        "  model = Conv2D(128, (5,5), strides=(2,2),kernel_initializer=init,padding='same')(model)\n",
        "  \n",
        "  model = LeakyReLU(alpha=0.2)(model)\n",
        "  \n",
        "  # downsample to 8x8\n",
        "  model = Conv2D(256, (5,5), strides=(2,2), kernel_initializer=init,padding='same')(model)\n",
        "  \n",
        "  model = LeakyReLU(alpha=0.2)(model)\n",
        "  \n",
        "\n",
        "  # downsample to 4x4\n",
        "  model = Conv2D(512, (5,5), strides=(2,2), padding='same')(model)\n",
        "  \n",
        "  model = LeakyReLU(alpha=0.2)(model)\n",
        "  model = Dropout(0.4)(model)\n",
        "  \n",
        "  # classifier\n",
        "  model = Flatten()(model)\n",
        "  \n",
        "\n",
        "  out1 = Dense(1, activation='sigmoid')(model)\n",
        "  \n",
        "  # compile model\n",
        "  model_fin = Model([in_image,in_label], out1)\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  \n",
        "  model_fin.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  \n",
        "  return model_fin,opt\n",
        "\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_gwNeWuTG_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim,n_labels = 40):\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    in_label = Input(shape=(n_labels,))\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    in_latent = Input(shape=(latent_dim,))\n",
        "    \n",
        "    #concatenate latent vector and label\n",
        "    x = concatenate([in_latent,in_label])\n",
        "    \n",
        "\n",
        "    # foundation for 4X4 feature maps\n",
        "\n",
        "    n_nodes = 512 * 4 * 4\n",
        "    dim_inp = latent_dim + n_labels \n",
        "    model = Dense(n_nodes, input_dim=dim_inp,kernel_initializer= init)(x)\n",
        "    \n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "   \n",
        "    model = Reshape((4, 4, 512))(model)\n",
        "    # upsample to 8X8\n",
        "   \n",
        "    model = Conv2DTranspose(256, (4,4), strides=(2,2), padding='same',kernel_initializer= init)(model)\n",
        "    \n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # upsample to 16x16\n",
        "    model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer= init)(model)\n",
        "    \n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # upsample to 32x32\n",
        "    model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer= init)(model)\n",
        "    \n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # upsample to 64x64\n",
        "    model = Conv2DTranspose(64, (4,4),strides=(2,2),padding='same',kernel_initializer= init)(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "\n",
        "    \n",
        "    \n",
        "    # output layer 64x64x3\n",
        "    model = Conv2D(3, (5,5),padding='same',kernel_initializer= init)(model)\n",
        "    \n",
        "    out = Activation('tanh')(model)\n",
        "\n",
        "    model_fin = Model([in_latent, in_label], out)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return model_fin"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLhD1biNTI2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "  # make weights in the discriminator not trainable\n",
        "  d_model.trainable = False\n",
        "  # connect them\n",
        "  gen_noise, gen_label = g_model.input\n",
        "  # add generator\n",
        "  gen_output = g_model.output\n",
        "  # add the discriminator\n",
        "  gan_output = d_model([gen_output,gen_label])\n",
        "  # compile model\n",
        "  \n",
        "  model = Model([gen_noise,gen_label], gan_output)\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "  optimizer=opt)  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdgRE_76TLej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select real samples\n",
        "import copy\n",
        "def generate_real_samples(dataset,label, n_samples):\n",
        "  # choose random instances\n",
        "  ix = randint(0, dataset.shape[0], n_samples)\n",
        "  # retrieve selected images with resepctive labels\n",
        "  X, labels = dataset[ix], label[ix]\n",
        "  \n",
        "  X = (X.astype('float32')-127.5)/127.5\n",
        "  \n",
        "  y = ones((n_samples, 1))\n",
        "  return [X,labels], y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82sC_SydTMxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim,labels_dataset, n_samples, n_labels = 40):\n",
        "  \n",
        "  # generate points in the latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  \n",
        "  # reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "\n",
        "  #select random labels from the dataset\n",
        "  idx = randint(0, labels_dataset.shape[0], n_samples)\n",
        "  labels = labels_dataset[idx]\n",
        "  \n",
        "  \n",
        "  return [x_input,labels]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlHnsZDDTPW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the generator to generate n fake examples, with labels\n",
        "def generate_fake_samples(g_model, latent_dim,labels_dataset, n_samples,n_labels = 40):\n",
        "  \n",
        "  # generate points in latent space\n",
        "  x_input,labels = generate_latent_points(latent_dim,labels_dataset, n_samples,n_labels)\n",
        "  \n",
        "  # predict outputs\n",
        "  X = g_model.predict([x_input,labels])\n",
        "  \n",
        "  # create fake class labels \n",
        "  y = zeros((n_samples, 1)) \n",
        "  return [X,labels], y"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9tnL3-DTRJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create and save a plot of generated images\n",
        "def save_plot(examples, epoch,labels_fake, n=4):\n",
        "  # scale from [-1,1] to [0,1]\n",
        "  examples = (examples + 1) / 2.0\n",
        "  # plot images\n",
        "  for i in range(n * n):\n",
        "  # define subplot\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "  # turn off axis\n",
        "    pyplot.axis('off')\n",
        "  # plot raw pixel data\n",
        "    pyplot.imshow(examples[i])\n",
        "  # save plot to file\n",
        "  labels_fake = np.array(labels_fake)\n",
        "  filename = 'drive/My Drive/model/fakelabelsACgan_e%03d.csv' % (epoch+1)\n",
        "  np.savetxt(filename, labels_fake, delimiter=\",\")\n",
        "  filename = 'drive/My Drive/model/generated_plotACgan_e%03d.png' % (epoch+1)\n",
        "  pyplot.savefig(filename)\n",
        "  pyplot.close()\n",
        "  # evaluate the discriminator, plot generated images, save generator model\n",
        "\n",
        "#generate and save a plot of fake images, save models\n",
        "def save_training(epoch, g_model,d_model,gan_model, dataset,labels_dataset, latent_dim,losses, n_samples=16):\n",
        "  # prepare real samples\n",
        "  [x_fake,labels_fake], y_real = generate_fake_samples(g_model,latent_dim,labels_dataset, n_samples)\n",
        "  # save plot\n",
        "\n",
        "  save_plot(x_fake, epoch,labels_fake)\n",
        "  # save the generator model tile file\n",
        "  filename = 'drive/My Drive/model/ACgenerator_model_%03d.h5' % (epoch + 1)\n",
        "  g_model.save(filename)\n",
        "  filename = 'drive/My Drive/model/ACgan_model_%03d.h5' % (1)\n",
        "  gan_model.save(filename)\n",
        "  filename = 'drive/My Drive/model/ACd_model_%03d.h5' % (1)\n",
        "  d_model.trainable = True\n",
        "  for layer in d_model.layers:\n",
        "    layer.trainable = True\n",
        "  d_model.save(filename)\n",
        "  d_model.trainable = False\n",
        "  for layer in d_model.layers:\n",
        "    layer.trainable = False\n",
        "  \n",
        "  np.savez('drive/My Drive/Losses/losses_modelfinalpre2.npz', *losses)\n",
        "  \n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJShBr4ob0uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training\n",
        "def train(g_model, d_model, gan_model, dataset,labels_dataset,latent_dim, n_epochs=40, n_batch=64):\n",
        "  half_batch = int(n_batch / 2)\n",
        "  bat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "  \n",
        "  dloss_real = []\n",
        "  dloss_fake = []\n",
        "  gloss = []\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "    # enumerate batches over the training set\n",
        "    for j in range(bat_per_epo):\n",
        "      # get randomly selected ✬real✬ samples\n",
        "      \n",
        "      [X_real,labels_real], y_real = generate_real_samples(dataset,labels_dataset, half_batch)\n",
        "      \n",
        "      # update discriminator model weights\n",
        "      d_loss1 = d_model.train_on_batch([X_real,labels_real],y_real)\n",
        "      \n",
        "      # generate fake examples\n",
        "      [X_fake,labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, labels_dataset, half_batch)\n",
        "      \n",
        "      # update discriminator model weights\n",
        "      d_loss2 = d_model.train_on_batch([X_fake,labels_fake],y_fake )\n",
        "      \n",
        "      # prepare points in latent space as input for the generator\n",
        "      [X_gan,label_gan] = generate_latent_points(latent_dim,labels_dataset, n_batch)\n",
        "      \n",
        "      # create inverted labels for the fake samples\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      \n",
        "      # update the generator via the discriminator's error\n",
        "      g_loss = gan_model.train_on_batch([X_gan,label_gan], [y_gan])\n",
        "      \n",
        "      #store losses\n",
        "      dloss_real.append(d_loss1)\n",
        "      dloss_fake.append(d_loss2)\n",
        "      gloss.append(g_loss)\n",
        "      \n",
        "\n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f' %\n",
        "      (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "      \n",
        "    \n",
        "      \n",
        "    losses = [np.array(dloss_real),np.array(dloss_fake),np.array(gloss)]\n",
        "    \n",
        "    save_training(i,g_model,d_model,gan_model, dataset, labels_dataset, latent_dim,losses)\n",
        "    \n",
        "  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1yvtUAfj-nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model,optd = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "\n",
        "\n",
        "# train model\n",
        "train(g_model, d_model, gan_model, celeba[:192599], attribute[:192599], latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA6X2KPvxZuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#function used to load models and restart training\n",
        "def load_models():\n",
        "    \n",
        "    \n",
        "    filename = 'drive/My Drive/model/ACd_model_001.h5'\n",
        "    d_model = load_model(filename, compile=True)\n",
        "    d_model.trainable = True\n",
        "    for layer in d_model.layers:\n",
        "        layer.trainable = True\n",
        "    d_model.summary()\n",
        "    filename = 'drive/My Drive/model/ACgenerator_model_035.h5'\n",
        "    g_model = load_model(filename, compile=True)\n",
        "    g_model.summary()\n",
        "    gan_model = define_gan(g_model, d_model)\n",
        "    gan_model.summary()\n",
        "    return d_model, g_model, gan_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}